<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Developing and PreTraining Large Language Models from Scratch: Scaling, Optimization, and Performance Analysis | Vineeth Veetil </title> <meta name="author" content="Vineeth Veetil"> <meta name="description" content="This project aims to implement &amp; pretrain a custom LLMs from scratch, starting with GPT-2"> <meta name="keywords" content="llm, ai, generative-ai, deep-learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://veetil314.github.io/projects/6_project/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Vineeth</span> Veetil </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Developing and PreTraining Large Language Models from Scratch: Scaling, Optimization, and Performance Analysis</h1> <p class="post-description">This project aims to implement &amp; pretrain a custom LLMs from scratch, starting with GPT-2</p> </header> <article> <h2 id="project-overview">Project Overview</h2> <p>This project aims to design and implement large language models (LLMs) from scratch using PyTorch, starting with GPT-2. The primary objectives are to explore architectural choices (ex MoE, rotary embeddings, SwiGLU), techniques for optimizing performance and to study scaling properties.</p> <p>Due to the high cost of GPU resources on popular cloud platforms like Google Cloud and AWS, the training will be conducted using Lambda Labs and VAST.ai, a cost-effective alternative, on 8 NVIDIA A100 GPUs. Vast.ai is good for open-source projects that do not involve proprietary data.</p> <h2 id="research-objectives">Research Objectives</h2> <ol> <li>Design and implement custom LLM models from scratch using PyTorch, focusing on exploring architectures and optimization techniques.</li> <li>Pretrain the model from scratch on open web data.</li> <li>Analyze the relationship between model size, computational resources, and performance, contributing to the understanding of scaling laws in language models.</li> <li>Employ techniques such as Flash Attention and Distributed Data Parallel (DDP) for high-speed distributed training.</li> <li>Study impact of architectural choices, such as SwiGLU, RMSNorm, Rotary embedding, Group Query Attention.</li> </ol> <h2 id="methodology">Methodology</h2> <h3 id="phase-1-model-design-and-implementation">Phase 1: Model Design and Implementation</h3> <ul> <li>Design the model architecture from scratch using PyTorch. Hand code modules to enable flexibility in exploration of architectures.</li> <li>Start with GPT-2, following literature from OpenAI to adopt similar architectures and initialization techniques to benchmark against.</li> </ul> <h3 id="phase-2-distributed-computing-and-optimization">Phase 2: Distributed Computing and Optimization</h3> <ul> <li>Integrate DDP into the model implementation to distribute the computational load across multiple GPUs.</li> <li>Use Flash Attention, a novel attention mechanism, to speed up the training process and improve the model’s efficiency.</li> <li>Optimize the model’s performance by fine-tuning hyperparameters and exploring techniques such as gradient accumulation and mixed-precision training.</li> </ul> <h3 id="phase-3-data-preparation-and-training">Phase 3: Data Preparation and Training</h3> <ul> <li>Collect and preprocess open web data suitable for training the GPT-2 model.</li> <li>Set up the training environment on VAST.ai, leveraging 8 NVIDIA A100 GPUs for efficient computation.</li> <li>Train the model on the prepared data, monitoring the validation loss over epochs to assess the model’s learning progress.</li> </ul> <h3 id="phase-4-performance-analysis-scaling-laws-and-exploring-different-architectures">Phase 4: Performance Analysis, Scaling Laws and Exploring Different Architectures</h3> <ul> <li>Analyze the relationship between model size, computational resources, and performance, considering factors such as training time, memory usage, and validation loss.</li> <li>Investigate the scaling laws governing the performance of language models, comparing the results obtained from the custom GPT-2 model with existing literature and benchmarks.</li> <li>Explore potential avenues for further scaling the model, such as increasing the number of parameters or adopting architectural choices such as MoE, SwiGLU, RMSNorm, Rotary embedding, Group Query Attention.</li> </ul> <h2 id="expected-outcomes">Expected Outcomes</h2> <ul> <li>Fully functional custom LLMs implemented from scratch in PyTorch</li> <li>Insights into the impact of model size on performance and computational requirements, contributing to the understanding of scaling laws.</li> <li>A comprehensive analysis of the model’s performance, including validation loss plots and comparisons with existing benchmarks.</li> </ul> <h2 id="future-directions">Future Directions</h2> <ul> <li>Investigate the impact of increasing the model size to 3 billion parameters and beyond on performance and computational requirements.</li> <li>Investigate other techniques for reducing the model’s computational footprint, such as knowledge distillation.</li> <li>Collaborate with the research community to contribute to the development of more efficient and scalable language models.</li> </ul> <p>By undertaking this project, we aim to deepen our understanding of LLM architectures and its scaling properties, while also contributing to the broader field of language model development. The insights gained from this project will inform future research directions and help in the development of more advanced and efficient language models.</p> <h2 id="references">References</h2> <p>https://github.com/huggingface/transformers/tree/main/src/transformers/models/gpt2 https://github.com/karpathy/nanoGPT</p> <p>© Copyright 2024 Vineeth Veetil. Hosted by GitHub Pages.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Vineeth Veetil. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>